{"cells":[{"cell_type":"code","execution_count":3,"source":["import numpy as np\n","#utilizaremos o pandas para lidar com o arquivo csv do dataset\n","import pandas as pd\n","#utilizaremos funções utilitárias do scikit learn para preparar os dados\n","from sklearn.preprocessing import LabelEncoder, MinMaxScaler, OneHotEncoder"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":4,"source":["#importamos o dataset e exibimos suas 5 primeiras linhas com a função \"head\"\n","skyserver_df = pd.read_csv('input/Skyserver_SQL2_27_2018 6_51_39 PM.csv')\n","skyserver_df.head()"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["          objid          ra       dec         u         g         r         i  \\\n","0  1.237650e+18  183.531326  0.089693  19.47406  17.04240  15.94699  15.50342   \n","1  1.237650e+18  183.598371  0.135285  18.66280  17.21449  16.67637  16.48922   \n","2  1.237650e+18  183.680207  0.126185  19.38298  18.19169  17.47428  17.08732   \n","3  1.237650e+18  183.870529  0.049911  17.76536  16.60272  16.16116  15.98233   \n","4  1.237650e+18  183.883288  0.102557  17.55025  16.26342  16.43869  16.55492   \n","\n","          z  run  rerun  camcol  field     specobjid   class  redshift  plate  \\\n","0  15.22531  752    301       4    267  3.722360e+18    STAR -0.000009   3306   \n","1  16.39150  752    301       4    267  3.638140e+17    STAR -0.000055    323   \n","2  16.80125  752    301       4    268  3.232740e+17  GALAXY  0.123111    287   \n","3  15.90438  752    301       4    269  3.722370e+18    STAR -0.000111   3306   \n","4  16.61326  752    301       4    269  3.722370e+18    STAR  0.000590   3306   \n","\n","     mjd  fiberid  \n","0  54922      491  \n","1  51615      541  \n","2  52023      513  \n","3  54922      510  \n","4  54922      512  "],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>objid</th>\n","      <th>ra</th>\n","      <th>dec</th>\n","      <th>u</th>\n","      <th>g</th>\n","      <th>r</th>\n","      <th>i</th>\n","      <th>z</th>\n","      <th>run</th>\n","      <th>rerun</th>\n","      <th>camcol</th>\n","      <th>field</th>\n","      <th>specobjid</th>\n","      <th>class</th>\n","      <th>redshift</th>\n","      <th>plate</th>\n","      <th>mjd</th>\n","      <th>fiberid</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1.237650e+18</td>\n","      <td>183.531326</td>\n","      <td>0.089693</td>\n","      <td>19.47406</td>\n","      <td>17.04240</td>\n","      <td>15.94699</td>\n","      <td>15.50342</td>\n","      <td>15.22531</td>\n","      <td>752</td>\n","      <td>301</td>\n","      <td>4</td>\n","      <td>267</td>\n","      <td>3.722360e+18</td>\n","      <td>STAR</td>\n","      <td>-0.000009</td>\n","      <td>3306</td>\n","      <td>54922</td>\n","      <td>491</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1.237650e+18</td>\n","      <td>183.598371</td>\n","      <td>0.135285</td>\n","      <td>18.66280</td>\n","      <td>17.21449</td>\n","      <td>16.67637</td>\n","      <td>16.48922</td>\n","      <td>16.39150</td>\n","      <td>752</td>\n","      <td>301</td>\n","      <td>4</td>\n","      <td>267</td>\n","      <td>3.638140e+17</td>\n","      <td>STAR</td>\n","      <td>-0.000055</td>\n","      <td>323</td>\n","      <td>51615</td>\n","      <td>541</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1.237650e+18</td>\n","      <td>183.680207</td>\n","      <td>0.126185</td>\n","      <td>19.38298</td>\n","      <td>18.19169</td>\n","      <td>17.47428</td>\n","      <td>17.08732</td>\n","      <td>16.80125</td>\n","      <td>752</td>\n","      <td>301</td>\n","      <td>4</td>\n","      <td>268</td>\n","      <td>3.232740e+17</td>\n","      <td>GALAXY</td>\n","      <td>0.123111</td>\n","      <td>287</td>\n","      <td>52023</td>\n","      <td>513</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1.237650e+18</td>\n","      <td>183.870529</td>\n","      <td>0.049911</td>\n","      <td>17.76536</td>\n","      <td>16.60272</td>\n","      <td>16.16116</td>\n","      <td>15.98233</td>\n","      <td>15.90438</td>\n","      <td>752</td>\n","      <td>301</td>\n","      <td>4</td>\n","      <td>269</td>\n","      <td>3.722370e+18</td>\n","      <td>STAR</td>\n","      <td>-0.000111</td>\n","      <td>3306</td>\n","      <td>54922</td>\n","      <td>510</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1.237650e+18</td>\n","      <td>183.883288</td>\n","      <td>0.102557</td>\n","      <td>17.55025</td>\n","      <td>16.26342</td>\n","      <td>16.43869</td>\n","      <td>16.55492</td>\n","      <td>16.61326</td>\n","      <td>752</td>\n","      <td>301</td>\n","      <td>4</td>\n","      <td>269</td>\n","      <td>3.722370e+18</td>\n","      <td>STAR</td>\n","      <td>0.000590</td>\n","      <td>3306</td>\n","      <td>54922</td>\n","      <td>512</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"]},"metadata":{},"execution_count":4}],"metadata":{"scrolled":true,"trusted":true,"_uuid":"078693a0f2b75505cc4e2db6c235c5919ac5ad10"}},{"cell_type":"code","execution_count":5,"source":["###TO-DO: CHECAR CORRRELAÇÃO DE DADOS\n","#eliminamos as colunas com dados irrelevantes à classificação\n","skyserver_df.drop(['objid', 'run', 'rerun', 'camcol', 'field', 'specobjid'], axis=1, inplace=True)"],"outputs":[],"metadata":{"scrolled":true,"trusted":true,"_uuid":"6faf9bf74427fbc77a4563e4225ec43ec795da9f"}},{"cell_type":"code","execution_count":6,"source":["#usamos LabelEncoder para transformar as classes categóricas em numéricas\n","le = LabelEncoder()\n","#o LabelEncoder é aplicado sobre o dataset\n","y_encoded = le.fit_transform(skyserver_df['class'])\n","#visto que as classes estão na variável y_encoded, podemos eliminar a coluna \"class\" do dataset\n","skyserver_df.drop(['class'], axis=1, inplace=True)\n","\n","print(np.shape(y_encoded))\n","y_encoded = y_encoded.reshape(-1, 1)\n","print(np.shape(y_encoded))\n","\n","enc = OneHotEncoder()\n","enc.fit(y_encoded)\n","y_encoded = enc.transform(y_encoded)\n","y_encoded = y_encoded.toarray().tolist()"],"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'np' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-484e95f35a64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mskyserver_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_encoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0my_encoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_encoded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_encoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"]}],"metadata":{"trusted":true,"_uuid":"b0dc8c20f488860d47cca6eee76944188cf93d6d"}},{"cell_type":"code","execution_count":null,"source":["scaler = MinMaxScaler(feature_range=(-1, 1))\n","sdss = scaler.fit_transform(sdss_df).tolist()"],"outputs":[],"metadata":{"trusted":true,"_uuid":"4ee637b6e756f4e741096347cfbb070dea0063da"}},{"cell_type":"code","execution_count":null,"source":["import numpy as np\n","\n","def stitch(inputs, target):\n","    series_inputs = pd.Series(inputs)\n","    series_target = pd.Series(target)\n","    df = pd.DataFrame({'inputs': series_inputs, 'target': series_target})\n","    return df.values.tolist()\n","\n","def unstitch(data):\n","    df = pd.DataFrame(data)\n","    return df[0].values.tolist(), df[1].values.tolist()"],"outputs":[],"metadata":{"scrolled":true,"trusted":true,"_uuid":"b13bf973bda0089164bca47f6783b85eea8a3199"}},{"cell_type":"code","execution_count":null,"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.autograd import Variable\n","\n","class Net(nn.Module):\n","    def __init__(self, dropout=False, weightDecay=0):\n","        super(Net, self).__init__()\n","        self.fc1 = nn.Linear(11, 11) # 2 Input noses, 50 in middle layers\n","        self.do1 = nn.Dropout(p=0.2)\n","        self.rl1 = nn.Sigmoid()\n","        self.fc2 = nn.Linear(11, 3)\n","        self.do2 = nn.Dropout(p=0.2)\n","        self.smout = nn.Softmax(dim=1)      \n","        \n","        self.criterion = nn.CrossEntropyLoss()\n","        self.optimizer = optim.Adam(self.parameters(), lr=0.01, weight_decay=0)\n","        self.dropout=dropout\n","        \n","        self.cuda()\n","        \n","        self.trainOverTimeAccuracy = []\n","        self.trainOverTimeLoss = []\n","        self.testOverTimeAccuracy = []\n","        self.testOverTimeLoss = []\n","    \n","    def forward(self, x):\n","        x = self.fc1(x)\n","        if self.dropout:\n","            x = self.do1(x)\n","        x = self.rl1(x)\n","        x = self.fc2(x)\n","        if self.dropout:\n","            x = self.do1(x)\n","        x = self.smout(x)\n","        return x\n","    \n","    def log(self, epoch, train, test):\n","        if epoch % REPORT_RATE == 0:\n","            self.trainOverTimeAccuracy.append(self.accuracy(train))\n","            #self.trainOverTimeLoss.append(self.loss(train))\n","            self.testOverTimeAccuracy.append(self.accuracy(test))\n","            #self.testOverTimeLoss.append(self.loss(test))\n","            \n","    def epochTrain(self, inputs, labels):\n","        self.optimizer.zero_grad()\n","        outputs = self(inputs)\n","        self.loss = self.criterion(outputs, torch.max(labels, 1)[1])\n","        self.loss.backward()    \n","        self.optimizer.step()\n","\n","        \n","    def train(self, numEpochs, train_set, train, test):\n","        inputs, labels = unstitch(train_set)\n","        \n","        inputs = Variable(torch.FloatTensor(inputs).cuda())\n","        labels = Variable(torch.FloatTensor(labels).cuda())\n","\n","        for epoch in range(numEpochs):\n","            self.epochTrain(inputs, labels)\n","            self.log(epoch, train, test)\n","                \n","    def activetrain(self, activeUpdateRate, batchSize, numEpochs, train, test):\n","        trainings = int(numEpochs/activeUpdateRate)\n","        currentEpoch = 0\n","\n","        for i in range(trainings):\n","            activecriterion = nn.CrossEntropyLoss(reduction='none')\n","            \n","            inputs, labels = unstitch(train)\n","            \n","            inputs = Variable(torch.FloatTensor(inputs).cuda())\n","            labels = Variable(torch.FloatTensor(labels).cuda())\n","            \n","            outputs = self(inputs)\n","\n","            loss = activecriterion(outputs, torch.max(labels, 1)[1]).detach().cpu().numpy()\n","            sortIndexs = np.argsort(loss)[::-1]\n","            dynamicTrainSet = np.array(np.copy(train))[sortIndexs]\n","            dynamicTrainSet = dynamicTrainSet[0:batchSize]\n","\n","            inputs, labels = unstitch(dynamicTrainSet)\n","        \n","            inputs = Variable(torch.FloatTensor(inputs).cuda())\n","            labels = Variable(torch.FloatTensor(labels).cuda())\n","            \n","            for epoch in range(activeUpdateRate):\n","                currentEpoch += 1\n","                self.epochTrain(inputs, labels)\n","                self.log(currentEpoch, train, test)\n","            \n","    def randomtrain(self, batchSize, numEpochs, train, test):\n","        shuffledTrainSet = np.array(np.copy(train))\n","        for epoch in range(numEpochs):\n","            np.random.shuffle(shuffledTrainSet)\n","\n","            inputs, labels = unstitch(shuffledTrainSet[0:batchSize])\n","\n","            inputs = Variable(torch.FloatTensor(inputs).cuda())\n","            labels = Variable(torch.FloatTensor(labels).cuda())\n","            \n","            self.epochTrain(inputs, labels)\n","            self.log(epoch, train, test)\n","\n","            \n","    def loss(self, test_set):\n","        inputs, labels = unstitch(test_set)\n","        inputs = Variable(torch.FloatTensor(inputs).cuda())\n","        labels = Variable(torch.FloatTensor(labels).cuda())\n","        result = self(inputs)\n","        loss = self.criterion(result, torch.max(labels, 1)[1])\n","\n","        return loss.item()\n","\n","    def accuracy(self, test_set):\n","        inputs, labels = unstitch(test_set)\n","        result = self(Variable(torch.FloatTensor(inputs)).cuda())\n","        inputs_max = np.argmax(result.detach().cpu().numpy(), axis=1)\n","        labels_max = np.argmax(np.array(labels), axis=1)\n","        correct = np.sum(inputs_max == labels_max)\n","\n","        return correct/len(test_set)"],"outputs":[],"metadata":{"trusted":true,"_uuid":"f148746d3ef7f7ccf43af24daeba55eb1a0236b3"}},{"cell_type":"code","execution_count":null,"source":["print(len(sdss), len(y_encoded))\n","data = stitch(sdss, y_encoded)"],"outputs":[],"metadata":{"trusted":true,"_uuid":"a4282307bdf9ca85b8182e228b8f200eac0bfdd2"}},{"cell_type":"code","execution_count":null,"source":["from sklearn.model_selection import train_test_split, ShuffleSplit\n","\n","testSize = 0.2\n","valSize = 0.2\n","trainSize = 1.0 - (testSize + valSize)\n","subPercentage = 0.25\n","\n","SAMPLES = 30\n","NUM_EPOCHS = 2000\n","BATCH_SIZE = 200\n","ACTIVE_UPDATE_RATE = 1\n","REPORT_RATE = 10\n","\n","print(subPercentage)\n","\n","train, test = train_test_split(data, test_size=testSize, shuffle=True)\n","train, val = train_test_split(train, test_size=subPercentage , shuffle=True)\n","\n","print(len(data))\n","print(len(train), len(test), len(val))"],"outputs":[],"metadata":{"trusted":true,"_uuid":"35d6527f1897ccde8423189e2843695ed37add36"}},{"cell_type":"code","execution_count":null,"source":["%%time\n","regularisationBatchNets = []\n","regularisationMiniBatchNets = []\n","regularisationActiveNets = []\n","\n","regularisationSchemes = [[False, 0, 'no regularisation'], [True, 0, 'dropout'], [False, 0.01, 'weight decay'], [False, 0.01, 'dropout & weight decay']]\n","\n","for scheme in regularisationSchemes:\n","    print(scheme[2])\n","    batchNets = []\n","    miniBatchNets = []\n","    activeNets = []\n","    for i in range(SAMPLES):\n","        batchNet = Net(scheme[0], scheme[1])\n","        batchNet.train(NUM_EPOCHS, train, train, test)\n","        batchNets.append(batchNet)\n","\n","        miniBatchNet = Net(scheme[0], scheme[1])\n","        miniBatchNet.randomtrain(BATCH_SIZE, NUM_EPOCHS, train, test)\n","        miniBatchNets.append(miniBatchNet)\n","\n","        activeNet = Net(scheme[0], scheme[1])\n","        activeNet.activetrain(ACTIVE_UPDATE_RATE, BATCH_SIZE, NUM_EPOCHS, train, test)\n","        activeNets.append(activeNet)\n","        \n","    regularisationBatchNets.append([batchNets, scheme[2]])\n","    regularisationMiniBatchNets.append([miniBatchNets, scheme[2]])\n","    regularisationActiveNets.append([activeNets, scheme[2]])"],"outputs":[],"metadata":{"scrolled":true,"trusted":true,"_uuid":"56d5bb12852720a3036c0be4981d28bf0cf9eb9e"}},{"cell_type":"code","execution_count":null,"source":["def buildDataFrame():\n","    meanTrainAccList = []\n","    meanTestAccList = []\n","    meanValAccList = []\n","    descriptionList = []\n","    \n","    padding = ['', '', '', '', '', '', '']\n","        \n","    typeList = []\n","    typeList.extend(['batch'])\n","    typeList.extend(padding)\n","    typeList.extend(['random mini batch'])\n","    typeList.extend(padding)\n","    typeList.extend(['selective learning'])\n","    typeList.extend(padding)\n","    for scheme in regularisationBatchNets:\n","        trainAccuracys = np.array(list(map(lambda x: x.accuracy(train), scheme[0])))\n","        meanTrainAccList.append(1.0 - np.mean(trainAccuracys))\n","        meanTrainAccList.append(np.std(trainAccuracys))\n","        \n","        testAccuracys = np.array(list(map(lambda x: x.accuracy(test), scheme[0])))\n","        meanTestAccList.append(1.0 - np.mean(testAccuracys))\n","        meanTestAccList.append(np.std(testAccuracys))\n","        \n","        valAccuracys = np.array(list(map(lambda x: x.accuracy(val), scheme[0])))\n","        meanValAccList.append(np.mean(testAccuracys / trainAccuracys))\n","        meanValAccList.append(np.std(testAccuracys / trainAccuracys))\n","        \n","        \n","        descriptionList.append(scheme[1])\n","        descriptionList.append('')\n","        \n","    for scheme in regularisationMiniBatchNets:\n","        trainAccuracys = np.array(list(map(lambda x: x.accuracy(train), scheme[0])))\n","        meanTrainAccList.append(1.0 - np.mean(trainAccuracys))\n","        meanTrainAccList.append(np.std(trainAccuracys))\n","        \n","        testAccuracys = np.array(list(map(lambda x: x.accuracy(test), scheme[0])))\n","        meanTestAccList.append(1.0 - np.mean(testAccuracys))\n","        meanTestAccList.append(np.std(testAccuracys))\n","        \n","        valAccuracys = np.array(list(map(lambda x: x.accuracy(val), scheme[0])))\n","        meanValAccList.append(np.mean(testAccuracys / trainAccuracys))\n","        meanValAccList.append(np.std(testAccuracys / trainAccuracys))\n","        \n","        \n","        descriptionList.append(scheme[1])\n","        descriptionList.append('')\n","        \n","    for scheme in regularisationActiveNets:\n","        trainAccuracys = np.array(list(map(lambda x: x.accuracy(train), scheme[0])))\n","        meanTrainAccList.append(1.0 - np.mean(trainAccuracys))\n","        meanTrainAccList.append(np.std(trainAccuracys))\n","        \n","        testAccuracys = np.array(list(map(lambda x: x.accuracy(test), scheme[0])))\n","        meanTestAccList.append(1.0 - np.mean(testAccuracys))\n","        meanTestAccList.append(np.std(testAccuracys))\n","        \n","        valAccuracys = np.array(list(map(lambda x: x.accuracy(val), scheme[0])))\n","        meanValAccList.append(np.mean(testAccuracys / trainAccuracys))\n","        meanValAccList.append(np.std(testAccuracys / trainAccuracys))\n","        \n","        \n","        descriptionList.append(scheme[1])\n","        descriptionList.append('')\n","        \n","    df = pd.DataFrame({'training': typeList, 'regularisation': descriptionList, '$trainError$': meanTrainAccList, '$testError$': meanTestAccList, '$genFactor$': meanValAccList})\n","    \n","    return df\n","\n","table = buildDataFrame()\n","table_result = table[['training', 'regularisation', '$trainError$', '$testError$', '$genFactor$']]\n","table_result"],"outputs":[],"metadata":{"trusted":true,"_uuid":"cc70a91e8ff11eec69f98526e7a485b0d86fd74c"}},{"cell_type":"code","execution_count":null,"source":["#print(table_result.to_latex(index=False, bold_rows=True, na_rep=''))\n","with open('./resulttable.txt', 'w') as f:\n","    print(table_result.to_latex(index=False, bold_rows=True, na_rep=''), file=f)"],"outputs":[],"metadata":{"trusted":true,"_uuid":"dddd0c043590d713011b472b90b0889f5290cdac"}},{"cell_type":"code","execution_count":null,"source":["import matplotlib.pyplot as plt\n","\n","# SMALL_SIZE = 10\n","# MEDIUM_SIZE = 12\n","\n","# plt.rc('font', size=SMALL_SIZE)\n","# plt.rc('axes', titlesize=MEDIUM_SIZE)\n","# plt.rc('axes', labelsize=MEDIUM_SIZE)\n","# plt.rcParams['figure.dpi']=150\n","\n","generations = np.arange(0, NUM_EPOCHS, REPORT_RATE)\n","\n","plotColors = [\n","    'b--',\n","    'r--',\n","    'g--',\n","    'k--',\n","    'g^',\n","    'k'\n","]\n","\n","\n","graphs = [[regularisationBatchNets, 'batch'], [regularisationMiniBatchNets, 'miniBatch'], [regularisationActiveNets, 'active']]\n","for graph in graphs:\n","    fig = plt.figure()\n","    plt.grid(1)\n","    plt.xlim([0, NUM_EPOCHS])\n","    plt.ion()\n","    plt.xlabel('Generations')\n","    plt.ylabel('Fitness')\n","    plots = []\n","    descriptions = []\n","    for x, result in enumerate(graph[0]):\n","        overTimeAccuracy = np.array(list(map(lambda x: x.trainOverTimeAccuracy, result[0])))\n","        meanOverTimeAccuracy = 1 - np.mean(overTimeAccuracy, axis=0)\n","        plots.append(plt.plot(generations, meanOverTimeAccuracy, plotColors[x%len(plotColors)] , linewidth=1, markersize=1)[0])\n","        descriptions.append(result[1])\n","\n","    plt.legend(plots, descriptions)\n","    fig.savefig('./' + graph[1] + 'Traning.png')\n","    plt.show(5)\n","\n","    plt.close()"],"outputs":[],"metadata":{"trusted":true,"_uuid":"1fd1a82cc61d5c20e74ae834573e2a5b2d925bc7"}},{"cell_type":"code","execution_count":null,"source":["fig = plt.figure()\n","plt.grid(1)\n","plt.xlim([0, NUM_EPOCHS])\n","plt.ion()\n","plt.xlabel('Generations')\n","plt.ylabel('Fitness')\n","plots = []\n","descriptions = []\n","\n","things = [[regularisationBatchNets, 'batch'], [regularisationMiniBatchNets, 'miniBatch'], [regularisationActiveNets, 'active']]\n","for x, graph in enumerate(things):\n","    \n","    overTimeAccuracy = np.array(list(map(lambda x: x.trainOverTimeAccuracy, graph[0][0][0])))\n","    meanOverTimeAccuracy = 1 - np.mean(overTimeAccuracy, axis=0)\n","    plots.append(plt.plot(generations, meanOverTimeAccuracy, plotColors[x%len(plotColors)] , linewidth=1, markersize=3)[0])\n","    descriptions.append(graph[1])\n","\n","plt.legend(plots, descriptions)\n","fig.savefig('./none.png')\n","plt.show(5)\n","\n","plt.close()"],"outputs":[],"metadata":{"trusted":true,"_uuid":"edc747bd6dca3ef4ef59d4e9cfa8235f9336b6e0"}}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3.8.3 64-bit ('deeplearning': conda)"},"language_info":{"name":"python","version":"3.8.3","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"interpreter":{"hash":"6c555ae357bbdb515d6d9686cab1868349dcab72626e98d0df5ed4f7e6a39c1b"}},"nbformat":4,"nbformat_minor":1}